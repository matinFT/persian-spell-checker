{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82306554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import parsivar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from polyleven import levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6edf97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = parsivar.Normalizer()\n",
    "tokenizer = parsivar.Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73562a91",
   "metadata": {},
   "source": [
    "# Keyhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aeb9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"Keyhan.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df667eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uniques = []\n",
    "content = None\n",
    "for key in data:\n",
    "    content = data[key][\"content\"]\n",
    "    uniques.extend([x for x in np.unique([y for y in normalizer.normalize(content)]) if x not in uniques])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c0f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "['\\n', ' ', '!', '(', ')', '.', '0', '1', '3', '5', '9', ':', '«', '\\xad', '»', '،', '؟', 'آ', 'ئ', 'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ل', 'م', 'ن', 'ه', 'و', 'پ', 'چ', 'ژ', 'ک', 'گ', 'ی', '\\u200c', '2', '4', '6', '8', '[', ']', '7', '؛', '|', 'ّ', 'ْ', '-', '٪', '#', '@', '_', 'a', 'd', 'e', 'f', 'i', 'j', 'm', 'n', 'r', 'y', 'A', 'I', 'N', 'U', '=', '٥', '*', '/', 'E', 'H', 'K', 'M', 'R', '%', 'B', 'C', 'G', 'O', 'F', '+', 'P', 'S', 'D', 'L', 'W', 'c', 'g', 'k', 'l', 'o', 'p', 't', 'u', 'v', 'w', 'T', 'b', 'h', 's', 'Ε', 'Ι', 'V', 'x', 'Z', '?', 'Y', 'J', 'X', 'Q', '&', 'z', 'ٔ', 'q', '×', ';', 'ʳ', '\"', '\\u200a', '`', '{', '}', 'ö', '÷', '\\u2005', 'é', 'ۂ', 'ٰ', '$', 'ü', 'Î', 'İ', 'è', 'ş', 'å', \"'\", '±', '\\u2001', 'ó', '\\u2009', 'â', 'ï', '°', '>', '\\\\']\n"
     ]
    }
   ],
   "source": [
    "print(len(uniques))\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6605c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حالت­خوبه\n",
      "حالت‌خوبه\n",
      "حالت خوبه\n",
      "حالت خوبه\n",
      "حالت خوبه\n",
      "حالت خوبه\n"
     ]
    }
   ],
   "source": [
    "print(\"حالت\"+\"\\xad\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200c\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200a\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2001\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2005\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2009\"+\"خوبه\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e858cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "    \"\\xad\":\"\",\n",
    "    \"\\u200a\":\" \",\n",
    "    \"\\u2001\":\" \",\n",
    "    \"\\u2005\":\" \",\n",
    "    \"\\u2009\":\" \",\n",
    "    \"ۂ\":\n",
    "        \"ه\",\n",
    "    \"ئ\": \n",
    "        \"ی\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47cc1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_characters = [0,1,2,3,4,5,6,7,8,9,\n",
    "                       \"و\", \"ن\", \"م\", \"ل\", \"گ\", \"ک\", \"ق\", \"ف\", \"غ\", \"ع\", \"ظ\", \"ط\", \"ض\", \"ص\", \"ی\", \"ه\",\n",
    "                       \"ش\", \"س\", \"ژ\", \"ز\", \"ر\", \"ذ\", \"د\", \"خ\", \"ح\", \"چ\", \"ج\", \"ث\", \"ت\", \"پ\", \"ب\", \"ا\", \"آ\",\n",
    "                       \" \", \"\\u200c\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "858abbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "f = open(\"test.txt\", \"w\", encoding='utf-8')\n",
    "for key in data:\n",
    "    for x in tokenizer.tokenize_sentences(normalizer.normalize(data[key][\"content\"].replace(\"\\n\", \".\"))):\n",
    "        res = \"\"\n",
    "        for character in x:\n",
    "            if character not in accepted_characters:\n",
    "                if character in replace_dict:\n",
    "                    res += replace_dict[character]\n",
    "            else:\n",
    "                res += character\n",
    "        f.write(res+\"\\n\")\n",
    "#         f.write(x+\"\\n\")\n",
    "        \n",
    "#     i += 1\n",
    "#     if i == 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d760981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55500 posts\n",
      "152491109 characters\n",
      "1182522 lines\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(data)} posts')\n",
    "f = open(\"test.txt\", \"r\", encoding='utf-8')\n",
    "number_of_chars = len(f.read())\n",
    "print(f'{number_of_chars} characters')\n",
    "f.close()\n",
    "f = open(\"test.txt\", \"r\", encoding='utf-8')\n",
    "number_of_lines = len(f.readlines())\n",
    "print(f'{number_of_lines} lines')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8882ea9f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_uniques = []\n",
    "content = None\n",
    "with open(\"test.txt\", \"r\", encoding='utf-8') as t:\n",
    "    while True:\n",
    "        content = t.readline()\n",
    "        if content == \"\":\n",
    "            break\n",
    "        text_uniques.extend([x for x in np.unique([y for y in content]) if x not in text_uniques])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bf87351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "['\\n', ' ', 'آ', 'ا', 'ت', 'ح', 'خ', 'د', 'ر', 'ز', 'س', 'ش', 'ع', 'ل', 'م', 'ن', 'ه', 'و', 'پ', 'چ', 'ک', 'ی', 'ب', 'ج', 'ص', 'غ', 'ف', 'ق', 'ذ', '\\u200c', 'ث', 'ض', 'ط', 'ژ', 'گ', 'ظ', '-']\n"
     ]
    }
   ],
   "source": [
    "print(len(text_uniques))\n",
    "print(text_uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe961752",
   "metadata": {},
   "source": [
    "# Hamshahri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04f96ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(\"Hamshahri.json\", \"r\"))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67096b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uniques = []\n",
    "content = None\n",
    "for key in data:\n",
    "    for key2 in data[key][\"posts\"]:\n",
    "        content = data[key][\"posts\"][key2]\n",
    "        uniques.extend([x for x in np.unique([y for y in normalizer.normalize(content)]) if x not in uniques])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46caf6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "['\\n', ' ', '-', '.', '0', '1', '2', '3', '4', '5', '7', '8', '9', ':', '«', '»', '،', '؛', 'آ', 'ئ', 'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'ف', 'ق', 'ل', 'م', 'ن', 'ه', 'و', 'پ', 'چ', 'ژ', 'ک', 'گ', 'ی', '\\u200c', '(', ')', '6', 'ذ', 'غ', 'E', 'T', 'U', 'S', '؟', '!', 'a', 'c', 'm', 's', 'A', 'B', 'J', 'M', 'N', 'O', 'R', 'V', 'e', 'i', 'n', 'p', '*', 'C', '/', 'F', 'D', 'P', 'ّ', 'd', 'f', 'h', 'k', 'l', 'o', 'r', 't', 'g', 'x', 'z', 'L', '#', 'I', 'Q', '+', 'W', '[', ']', '&', 'u', 'H', 'K', 'X', 'G', '\\u2009', 'v', 'y', 'b', '_', 'ٓ', '٥', 'ْ', '\\xad', 'Z', 'Y', 'w', '%', 'j', '@', '|', 'q', '\\u200a', 'í', '×', '{', '}', '>', '=', 'è', '?', 'é', \"'\", 'ٰ', 'ٔ', 'à', 'ç', 'ê', 'İ', 'ş', '\"', ';', 'ۆ', '\\\\', '٪', '\\x81', '\\u2005']\n"
     ]
    }
   ],
   "source": [
    "print(len(uniques))\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c61b01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "    \"\\xad\":\"\",\n",
    "    \"\\u200a\":\" \",\n",
    "    \"\\u2001\":\" \",\n",
    "    \"\\u2005\":\" \",\n",
    "    \"\\u2009\":\" \",\n",
    "    \"ۂ\":\n",
    "        \"ه\",\n",
    "    \"ئ\": \n",
    "        \"ی\",\n",
    "    \"٥\":\n",
    "        \"5\",\n",
    "    \"ۆ\": \n",
    "        \"و\",\n",
    "    \n",
    "}\n",
    "accepted_characters = [0,1,2,3,4,5,6,7,8,9,\n",
    "                       \"و\", \"ن\", \"م\", \"ل\", \"گ\", \"ک\", \"ق\", \"ف\", \"غ\", \"ع\", \"ظ\", \"ط\", \"ض\", \"ص\", \"ی\", \"ه\",\n",
    "                       \"ش\", \"س\", \"ژ\", \"ز\", \"ر\", \"ذ\", \"د\", \"خ\", \"ح\", \"چ\", \"ج\", \"ث\", \"ت\", \"پ\", \"ب\", \"ا\", \"آ\",\n",
    "                       \" \", \"\\u200c\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "470522ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حالت­خوبه\n",
      "حالتخوبه\n",
      "حالت‌خوبه\n",
      "حالت‍خوبه\n",
      "حالت‫خوبه\n",
      "حالت‬خوبه\n",
      "حالت خوبه\n",
      "حالت‪خوبه\n",
      "حالت خوبه\n",
      "حالت خوبه\n"
     ]
    }
   ],
   "source": [
    "print(\"حالت\"+\"\\xad\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\x81\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200c\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200d\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u202b\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u202c\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200a\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u202a\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2005\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2009\"+\"خوبه\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6114cc2d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f = open(\"test.txt\", \"a\", encoding='utf-8')\n",
    "# i = 0\n",
    "for key in data:\n",
    "    for k in data[key][\"posts\"]:\n",
    "        for x in tokenizer.tokenize_sentences(normalizer.normalize(data[key][\"posts\"][k].replace(\"\\n\", \" .\"))):\n",
    "            res = \"\"\n",
    "            for character in x:\n",
    "                if character not in accepted_characters:\n",
    "                    if character in replace_dict:\n",
    "                        res += replace_dict[character]\n",
    "                else:\n",
    "                    res += character\n",
    "            f.write(res+\"\\n\")\n",
    "#     i += 1\n",
    "#     if i == 20:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbe755",
   "metadata": {},
   "source": [
    "# build ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c123b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "word2vec_model = Word2Vec.load(\"Word2VecModel\")\n",
    "# FastText_model = FastText.load(\"FastTextModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "07d6a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_tuple(words, model):\n",
    "    return tuple([model.wv.key_to_index[words[i]] for i in range(len(words))])\n",
    "\n",
    "# seems some of the words in the text file is not present in model key_to_index\n",
    "def build_ngrams(n):\n",
    "    ngram_counts = {}\n",
    "    with open(\"test.txt\", \"r\", encoding='utf-8') as t:\n",
    "        while True:\n",
    "            line = t.readline()\n",
    "            if line == \"\":\n",
    "                break\n",
    "            words = tokenizer.tokenize_words(line)\n",
    "            for i in range(max(len(words)-n+1, 0)):\n",
    "                try:\n",
    "#                     pre_words_indices = tuple([word2vec_model.wv.key_to_index[words[i+j]] for j in range(n)])\n",
    "                    pre_words_indices = words_to_tuple(words[i:i+n], word2vec_model)\n",
    "                    if pre_words_indices in ngram_counts:\n",
    "                        ngram_counts[pre_words_indices] += 1\n",
    "                    else:\n",
    "                        ngram_counts[pre_words_indices] = 0\n",
    "                except:\n",
    "                    pass\n",
    "    return ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "27283c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words counts:  42671360\n",
      "biword counts:  34459834\n",
      "threeword counts:  19255364\n",
      "Wall time: 4min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_counts = build_ngrams(1)\n",
    "word_counts[\"all\"] = sum(word_counts.values())\n",
    "print(\"words counts: \", word_counts[\"all\"])\n",
    "# 100MB of RAM\n",
    "biword_counts = build_ngrams(2)\n",
    "biword_counts[\"all\"] = sum(biword_counts.values())\n",
    "print(\"biword counts: \", biword_counts[\"all\"])\n",
    "\n",
    "# 700MB of RAM\n",
    "threeword_counts = build_ngrams(3)\n",
    "threeword_counts[\"all\"] = sum(threeword_counts.values())\n",
    "print(\"threeword counts: \", threeword_counts[\"all\"])\n",
    "# 1.6GB of RAM\n",
    "# fourword_counts = build_ngrams(4)\n",
    "# fourword_counts[\"all\"] = sum(fourword_counts.values())\n",
    "# 3.5GB of RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c09e4",
   "metadata": {},
   "source": [
    "# make benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11e8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"faspell_main.csv\")\n",
    "# df = df[df[\"error-category\"] != 2].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9ffcde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_neighbors = {\n",
    "    \"ض\" : [\"ص\", \"ظ\", \"ز\", \"ذ\"],\n",
    "    \"ص\" : [\"ض\", \"ث\"],\n",
    "    \"ث\" : [\"ص\", \"ق\"],\n",
    "    \"ق\" : [\"ف\", \"ث\", \"غ\"],\n",
    "    \"ف\" : [\"ق\", \"غ\"],\n",
    "    \"غ\" : [\"ف\", \"ه\", \"ق\"],\n",
    "    \"ع\" : [\"غ\", \"ه\"],\n",
    "    \"ه\" : [\"ع\", \"خ\", \"ح\"],\n",
    "    \"خ\" : [\"ه\", \"ح\"],\n",
    "    \"ح\" : [\"خ\", \"ج\", \"ه\"],\n",
    "    \"ج\" : [\"ح\", \"چ\"],\n",
    "    \"چ\" : [\"ج\", \"پ\"],\n",
    "    \"پ\" : [\"چ\", \"ب\"],\n",
    "    \"ش\" : [\"س\"],\n",
    "    \"س\" : [\"ش\", \"ی\"],\n",
    "    \"ی\" : [\"س\", \"ب\"],\n",
    "    \"ب\" : [\"ی\", \"ل\"],\n",
    "    \"ل\" : [\"ب\", \"ا\", \"آ\"],\n",
    "    \"ا\" : [\"ل\", \"ت\"],\n",
    "    \"آ\" : [\"ل\", \"ت\"],\n",
    "    \"ت\" : [\"ا\", \"ن\", \"ط\", \"آ\"],\n",
    "    \"ن\" : [\"ت\", \"م\"],\n",
    "    \"م\" : [\"ن\", \"ک\"],\n",
    "    \"ک\" : [\"م\", \"گ\"],\n",
    "    \"گ\" : [\"ک\"],\n",
    "    \"ظ\" : [\"ط\", \"ض\", \"ز\", \"ذ\"],\n",
    "    \"ط\" : [\"ظ\", \"ز\", \"ت\"],\n",
    "    \"ز\" : [\"ط\", \"ض\", \"ر\", \"ذ\"],\n",
    "    \"ژ\" : [\"ط\", \"ر\"],\n",
    "    \"ر\" : [\"ز\", \"ذ\"],\n",
    "    \"ذ\" : [\"ر\", \"ض\", \"د\", \"ذ\"],\n",
    "    \"د\" : [\"ذ\", \"ی\"],\n",
    "    \"و\" : [\"د\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e757d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaceble_characters = [\"و\", \"ن\", \"م\", \"ل\", \"گ\", \"ک\", \"ق\", \"ف\", \"غ\", \"ع\", \"ظ\", \"ط\", \"ض\", \"ص\", \"ی\", \"ه\",\n",
    "#                         \"ش\", \"س\", \"ژ\", \"ز\", \"ر\", \"ذ\", \"د\", \"خ\", \"ح\", \"چ\", \"ج\", \"ث\", \"ت\", \"پ\", \"ب\", \"ا\", \"آ\"]\n",
    "\n",
    "replaceble_characters = list(character_neighbors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "806733fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [word2vec_model.wv.index_to_key[x[0][0]] for x in sorted(word_counts.items(), key = lambda x: x[1], reverse=True)[1:20]]\n",
    "stop_words_indices = [word2vec_model.wv.key_to_index[x] for x in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed7f82d",
   "metadata": {},
   "source": [
    "# random errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "879bf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_character_to_replace(character, close_change):\n",
    "    if close_change:\n",
    "        return random.choice(character_neighbors[character])\n",
    "    else:\n",
    "        return random.choice(list(set(replaceble_characters)-(set(character_neighbors[character]+[character]))))\n",
    "    \n",
    "def replace_character(phrase, location, close_change):\n",
    "    char = select_character_to_replace(phrase[location], close_change)\n",
    "    return phrase[:location] + char + phrase[location+1:]\n",
    "\n",
    "def del_character(phrase, location, close_change):\n",
    "    return phrase[:location] + phrase[location+1:]\n",
    "\n",
    "def add_character(phrase, location, close_change):\n",
    "    char = select_character_to_replace(phrase[location], close_change)\n",
    "    return phrase[:location] + char + phrase[location:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e9e2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_error_function = {\n",
    "    0:replace_character,\n",
    "    1:del_character,\n",
    "    2:add_character\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e47d8597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "fe1ff183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for replacement, 1 for deletion, 2 for addition\n",
    "def create_non_word_error(word, distance, close_change):\n",
    "    \n",
    "    while True:\n",
    "        locations = random.sample(range(len(word)), k=min(distance, len(word)))\n",
    "        if sum([word[location] in replaceble_characters for location in locations])==len(locations):\n",
    "            break\n",
    "\n",
    "    locations = np.array(sorted(locations))\n",
    "    changes = []\n",
    "    for i in range(len(locations)):\n",
    "        random_edit = random.randint(0, 2)\n",
    "        try:\n",
    "            word = index_to_error_function[random_edit](word, locations[i], close_change)\n",
    "        except Exception as e:\n",
    "            print(word)\n",
    "            print(locations[i])\n",
    "            print(distance)\n",
    "            print(word[location] not in replaceble_characters)\n",
    "            raise Exception()\n",
    "        if random_edit == 1:\n",
    "            if i != len(locations):\n",
    "                locations[i+1:] -= 1\n",
    "        elif random_edit == 2:\n",
    "            if i != len(locations):\n",
    "                locations[i+1:] += 1\n",
    "        changes.append(random_edit)\n",
    "    return word, changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a3d07a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices([1], k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "3a96b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_non_word_error(phrase, number_of_errors, number_of_words_with_error, close_change):\n",
    "    words = tokenizer.tokenize_words(normalizer.normalize(phrase))\n",
    "    final_words = []\n",
    "    final_changes = []\n",
    "    error_words_indices = random.choices(range(len(words)), k=number_of_words_with_error)\n",
    "    error_words_indices, word_error_counts = np.unique(random.choices(error_words_indices, k=number_of_errors), return_counts=True)\n",
    "    for i in range(len(error_words_indices)):\n",
    "        if sum([character in replaceble_characters for character in words[i]])<word_error_counts[i]:\n",
    "            return None, None\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        if i in error_words_indices:\n",
    "            error, changes = create_non_word_error(words[i], word_error_counts[np.where(error_words_indices==i)[0][0]], close_change)\n",
    "            final_words.append(error)\n",
    "            final_changes.append(changes)\n",
    "        else:\n",
    "            final_words.append(words[i])\n",
    "            final_changes.append([])\n",
    "            \n",
    "    return \" \".join(final_words), final_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "2c371f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomly_chosen_biwords = random.sample([x[0] for x in sorted(biword_counts.items(), key=lambda x: x[1], reverse=True)[:10000] if \n",
    "                                         x[0][0] not in stop_words_indices and  x[0][1] not in stop_words_indices], k=100)\n",
    "randomly_chosen_biwords = [word2vec_model.wv.index_to_key[x[0]] + \" \" + word2vec_model.wv.index_to_key[x[1]] \n",
    "                           for x in randomly_chosen_biwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "0b317128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly_chosen_biwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "521591b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'correct word':[], \"misspelled word\":[], \"total edit distance\":[], \n",
    "                   \"number of words with error\":[], \"close change\": [], \"changes\":[]})\n",
    "for biword in randomly_chosen_biwords:\n",
    "#     print(biword)\n",
    "    for error_num in [1, 2, 3]:\n",
    "        for errored_word_num in range(1, min(error_num+1, 3)):\n",
    "            for close_change in [False, True]:\n",
    "                for i in range(3):\n",
    "                    misspelled_phrase, changes = make_non_word_error(biword,error_num, errored_word_num, close_change)\n",
    "                    if misspelled_phrase==None:\n",
    "                        continue\n",
    "                    if levenshtein(misspelled_phrase, biword) == error_num:\n",
    "                        df = df.append(pd.DataFrame({'correct word':biword, 'misspelled word': misspelled_phrase,\n",
    "                                                     'total edit distance':error_num, 'number of words with error':errored_word_num,\n",
    "                                                     'close change':close_change, 'changes':[changes]})).reset_index(drop=True)\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "dbcacd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct word</th>\n",
       "      <th>misspelled word</th>\n",
       "      <th>total edit distance</th>\n",
       "      <th>number of words with error</th>\n",
       "      <th>close change</th>\n",
       "      <th>changes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>کار می‌کند</td>\n",
       "      <td>کار ‌مکند</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[], [1, 1, 2]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>بیان این‌که</td>\n",
       "      <td>لبان این‌که</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[2, 1], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>خروج آمریکا</td>\n",
       "      <td>خفطج آمریکا</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0, 0], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>آمریکا علیه</td>\n",
       "      <td>آریک علیه</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[1, 1], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>اگر دولت</td>\n",
       "      <td>اار دولت</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>همشهری گفت</td>\n",
       "      <td>خمشخهری گفت</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[0, 2], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>کاهش نرخ</td>\n",
       "      <td>هش نرخ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[1, 1], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>بار دیگر</td>\n",
       "      <td>بار دی</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[], [1, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>کشور وارد</td>\n",
       "      <td>کشور ولاد</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[], [2, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>نشست خبری</td>\n",
       "      <td>نشست حخی</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[], [2, 1, 1]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    correct word misspelled word  total edit distance  \\\n",
       "117   کار می‌کند       کار ‌مکند                  3.0   \n",
       "910  بیان این‌که     لبان این‌که                  2.0   \n",
       "774  خروج آمریکا     خفطج آمریکا                  2.0   \n",
       "674  آمریکا علیه       آریک علیه                  2.0   \n",
       "202     اگر دولت        اار دولت                  1.0   \n",
       "406   همشهری گفت     خمشخهری گفت                  2.0   \n",
       "132     کاهش نرخ          هش نرخ                  2.0   \n",
       "618     بار دیگر          بار دی                  2.0   \n",
       "103    کشور وارد       کشور ولاد                  2.0   \n",
       "452    نشست خبری        نشست حخی                  3.0   \n",
       "\n",
       "     number of words with error  close change          changes  \n",
       "117                         1.0           1.0  [[], [1, 1, 2]]  \n",
       "910                         2.0           1.0     [[2, 1], []]  \n",
       "774                         2.0           0.0     [[0, 0], []]  \n",
       "674                         2.0           0.0     [[1, 1], []]  \n",
       "202                         1.0           0.0        [[0], []]  \n",
       "406                         1.0           1.0     [[0, 2], []]  \n",
       "132                         1.0           1.0     [[1, 1], []]  \n",
       "618                         2.0           0.0     [[], [1, 1]]  \n",
       "103                         1.0           1.0     [[], [2, 1]]  \n",
       "452                         2.0           1.0  [[], [2, 1, 1]]  "
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "7b07ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"randomly misspelled bigram words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word, misspelled word, edit distance, number of words with error, close_change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f913128",
   "metadata": {},
   "source": [
    "# e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_bigrams = sorted(biword_counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "bbf28337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spell_errors(phrase):\n",
    "    misspelled_words = []\n",
    "    while True:\n",
    "        error_locations = random.sample(range(len(phrase)), k=2)\n",
    "        i=0\n",
    "        while i < len(error_locations):\n",
    "            if phrase[error_locations[i]] == \" \" or phrase[error_locations[i]] == \"\\u200c\" or phrase[error_locations[i]] == \"-\":\n",
    "                break\n",
    "            i+=1\n",
    "#             print(i)\n",
    "        if i == len(error_locations):\n",
    "            break\n",
    "    error_locations = list(sorted(error_locations))\n",
    "    for location in error_locations:\n",
    "        misspelled_words.extend([replace_character(phrase, location), add_character(phrase, location), del_character(phrase, location)])\n",
    "    misspelled_words.append(replace_character(replace_character(phrase, error_locations[0]), error_locations[1]))\n",
    "    misspelled_words.append(add_character(replace_character(phrase, error_locations[0]), error_locations[1]))\n",
    "    misspelled_words.append(add_character(replace_character(phrase, error_locations[0]), error_locations[1]))\n",
    "    \n",
    "    misspelled_words.append(replace_character(add_character(phrase, error_locations[0]), error_locations[1]+1))\n",
    "    misspelled_words.append(add_character(add_character(phrase, error_locations[0]), error_locations[1]+1))\n",
    "    misspelled_words.append(del_character(add_character(phrase, error_locations[0]), error_locations[1]+1))\n",
    "    \n",
    "    misspelled_words.append(replace_character(del_character(phrase, error_locations[0]), error_locations[1]-1))\n",
    "    misspelled_words.append(add_character(del_character(phrase, error_locations[0]), error_locations[1]-1))\n",
    "    misspelled_words.append(del_character(del_character(phrase, error_locations[0]), error_locations[1]-1))\n",
    "    return misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "e8a20b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams with frequency more than 100 and not containing stopwords\n",
    "top_bigrams_without_stopwords = [x for x in [x for x in sorted_bigrams if x[1] > 100] if x[0][0] not in stop_words_indices and x[0][1] not in stop_words_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "ba186faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_manipulate = [word2vec_model.wv.index_to_key[x[0][0]] + \" \" + word2vec_model.wv.index_to_key[x[0][1]] for x in random.sample(top_bigrams_without_stopwords, k=1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "4de62883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_to_manilupate[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b50fb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'correct word':[], \"misspelled word\":[]})\n",
    "for word in words_to_manilupate:\n",
    "    misspelled_words = create_spell_errors(word)\n",
    "    df = df.append(pd.DataFrame({'correct word':[word]*len(misspelled_words), 'misspelled word': misspelled_words})).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "14317304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "faef2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"misspelled bigram words.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
