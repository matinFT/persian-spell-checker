{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82306554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import parsivar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from polyleven import levenshtein\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6edf97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = parsivar.Normalizer()\n",
    "tokenizer = parsivar.Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73562a91",
   "metadata": {},
   "source": [
    "# Keyhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aeb9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"Keyhan.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df667eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uniques = []\n",
    "content = None\n",
    "for key in data:\n",
    "    content = data[key][\"content\"]\n",
    "    uniques.extend([x for x in np.unique([y for y in normalizer.normalize(content)]) if x not in uniques])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c0f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "['\\n', ' ', '!', '(', ')', '.', '0', '1', '3', '5', '9', ':', '«', '\\xad', '»', '،', '؟', 'آ', 'ئ', 'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ل', 'م', 'ن', 'ه', 'و', 'پ', 'چ', 'ژ', 'ک', 'گ', 'ی', '\\u200c', '2', '4', '6', '8', '[', ']', '7', '؛', '|', 'ّ', 'ْ', '-', '٪', '#', '@', '_', 'a', 'd', 'e', 'f', 'i', 'j', 'm', 'n', 'r', 'y', 'A', 'I', 'N', 'U', '=', '٥', '*', '/', 'E', 'H', 'K', 'M', 'R', '%', 'B', 'C', 'G', 'O', 'F', '+', 'P', 'S', 'D', 'L', 'W', 'c', 'g', 'k', 'l', 'o', 'p', 't', 'u', 'v', 'w', 'T', 'b', 'h', 's', 'Ε', 'Ι', 'V', 'x', 'Z', '?', 'Y', 'J', 'X', 'Q', '&', 'z', 'ٔ', 'q', '×', ';', 'ʳ', '\"', '\\u200a', '`', '{', '}', 'ö', '÷', '\\u2005', 'é', 'ۂ', 'ٰ', '$', 'ü', 'Î', 'İ', 'è', 'ş', 'å', \"'\", '±', '\\u2001', 'ó', '\\u2009', 'â', 'ï', '°', '>', '\\\\']\n"
     ]
    }
   ],
   "source": [
    "print(len(uniques))\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6605c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حالت­خوبه\n",
      "حالت‌خوبه\n",
      "حالت خوبه\n",
      "حالت خوبه\n",
      "حالت خوبه\n",
      "حالت خوبه\n"
     ]
    }
   ],
   "source": [
    "print(\"حالت\"+\"\\xad\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200c\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200a\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2001\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2005\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2009\"+\"خوبه\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e858cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "    \"\\xad\":\"\",\n",
    "    \"\\u200a\":\" \",\n",
    "    \"\\u2001\":\" \",\n",
    "    \"\\u2005\":\" \",\n",
    "    \"\\u2009\":\" \",\n",
    "    \"ۂ\":\n",
    "        \"ه\",\n",
    "    \"ئ\": \n",
    "        \"ی\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47cc1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_characters = [0,1,2,3,4,5,6,7,8,9,\n",
    "                       \"و\", \"ن\", \"م\", \"ل\", \"گ\", \"ک\", \"ق\", \"ف\", \"غ\", \"ع\", \"ظ\", \"ط\", \"ض\", \"ص\", \"ی\", \"ه\",\n",
    "                       \"ش\", \"س\", \"ژ\", \"ز\", \"ر\", \"ذ\", \"د\", \"خ\", \"ح\", \"چ\", \"ج\", \"ث\", \"ت\", \"پ\", \"ب\", \"ا\", \"آ\",\n",
    "                       \" \", \"\\u200c\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "858abbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "f = open(\"test.txt\", \"w\", encoding='utf-8')\n",
    "for key in data:\n",
    "    for x in tokenizer.tokenize_sentences(normalizer.normalize(data[key][\"content\"].replace(\"\\n\", \".\"))):\n",
    "        res = \"\"\n",
    "        for character in x:\n",
    "            if character not in accepted_characters:\n",
    "                if character in replace_dict:\n",
    "                    res += replace_dict[character]\n",
    "            else:\n",
    "                res += character\n",
    "        f.write(res+\"\\n\")\n",
    "#         f.write(x+\"\\n\")\n",
    "        \n",
    "#     i += 1\n",
    "#     if i == 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d760981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55500 posts\n",
      "152491109 characters\n",
      "1182522 lines\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(data)} posts')\n",
    "f = open(\"test.txt\", \"r\", encoding='utf-8')\n",
    "number_of_chars = len(f.read())\n",
    "print(f'{number_of_chars} characters')\n",
    "f.close()\n",
    "f = open(\"test.txt\", \"r\", encoding='utf-8')\n",
    "number_of_lines = len(f.readlines())\n",
    "print(f'{number_of_lines} lines')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8882ea9f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_uniques = []\n",
    "content = None\n",
    "with open(\"test.txt\", \"r\", encoding='utf-8') as t:\n",
    "    while True:\n",
    "        content = t.readline()\n",
    "        if content == \"\":\n",
    "            break\n",
    "        text_uniques.extend([x for x in np.unique([y for y in content]) if x not in text_uniques])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bf87351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "['\\n', ' ', 'آ', 'ا', 'ت', 'ح', 'خ', 'د', 'ر', 'ز', 'س', 'ش', 'ع', 'ل', 'م', 'ن', 'ه', 'و', 'پ', 'چ', 'ک', 'ی', 'ب', 'ج', 'ص', 'غ', 'ف', 'ق', 'ذ', '\\u200c', 'ث', 'ض', 'ط', 'ژ', 'گ', 'ظ', '-']\n"
     ]
    }
   ],
   "source": [
    "print(len(text_uniques))\n",
    "print(text_uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe961752",
   "metadata": {},
   "source": [
    "# Hamshahri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04f96ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(\"Hamshahri.json\", \"r\"))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67096b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uniques = []\n",
    "content = None\n",
    "for key in data:\n",
    "    for key2 in data[key][\"posts\"]:\n",
    "        content = data[key][\"posts\"][key2]\n",
    "        uniques.extend([x for x in np.unique([y for y in normalizer.normalize(content)]) if x not in uniques])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46caf6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "['\\n', ' ', '-', '.', '0', '1', '2', '3', '4', '5', '7', '8', '9', ':', '«', '»', '،', '؛', 'آ', 'ئ', 'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'ف', 'ق', 'ل', 'م', 'ن', 'ه', 'و', 'پ', 'چ', 'ژ', 'ک', 'گ', 'ی', '\\u200c', '(', ')', '6', 'ذ', 'غ', 'E', 'T', 'U', 'S', '؟', '!', 'a', 'c', 'm', 's', 'A', 'B', 'J', 'M', 'N', 'O', 'R', 'V', 'e', 'i', 'n', 'p', '*', 'C', '/', 'F', 'D', 'P', 'ّ', 'd', 'f', 'h', 'k', 'l', 'o', 'r', 't', 'g', 'x', 'z', 'L', '#', 'I', 'Q', '+', 'W', '[', ']', '&', 'u', 'H', 'K', 'X', 'G', '\\u2009', 'v', 'y', 'b', '_', 'ٓ', '٥', 'ْ', '\\xad', 'Z', 'Y', 'w', '%', 'j', '@', '|', 'q', '\\u200a', 'í', '×', '{', '}', '>', '=', 'è', '?', 'é', \"'\", 'ٰ', 'ٔ', 'à', 'ç', 'ê', 'İ', 'ş', '\"', ';', 'ۆ', '\\\\', '٪', '\\x81', '\\u2005']\n"
     ]
    }
   ],
   "source": [
    "print(len(uniques))\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c61b01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "    \"\\xad\":\"\",\n",
    "    \"\\u200a\":\" \",\n",
    "    \"\\u2001\":\" \",\n",
    "    \"\\u2005\":\" \",\n",
    "    \"\\u2009\":\" \",\n",
    "    \"ۂ\":\n",
    "        \"ه\",\n",
    "    \"ئ\": \n",
    "        \"ی\",\n",
    "    \"٥\":\n",
    "        \"5\",\n",
    "    \"ۆ\": \n",
    "        \"و\",\n",
    "    \n",
    "}\n",
    "accepted_characters = [0,1,2,3,4,5,6,7,8,9,\n",
    "                       \"و\", \"ن\", \"م\", \"ل\", \"گ\", \"ک\", \"ق\", \"ف\", \"غ\", \"ع\", \"ظ\", \"ط\", \"ض\", \"ص\", \"ی\", \"ه\",\n",
    "                       \"ش\", \"س\", \"ژ\", \"ز\", \"ر\", \"ذ\", \"د\", \"خ\", \"ح\", \"چ\", \"ج\", \"ث\", \"ت\", \"پ\", \"ب\", \"ا\", \"آ\",\n",
    "                       \" \", \"\\u200c\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "470522ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حالت­خوبه\n",
      "حالتخوبه\n",
      "حالت‌خوبه\n",
      "حالت‍خوبه\n",
      "حالت‫خوبه\n",
      "حالت‬خوبه\n",
      "حالت خوبه\n",
      "حالت‪خوبه\n",
      "حالت خوبه\n",
      "حالت خوبه\n"
     ]
    }
   ],
   "source": [
    "print(\"حالت\"+\"\\xad\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\x81\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200c\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200d\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u202b\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u202c\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200a\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u202a\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2005\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2009\"+\"خوبه\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6114cc2d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f = open(\"test.txt\", \"a\", encoding='utf-8')\n",
    "# i = 0\n",
    "for key in data:\n",
    "    for k in data[key][\"posts\"]:\n",
    "        for x in tokenizer.tokenize_sentences(normalizer.normalize(data[key][\"posts\"][k].replace(\"\\n\", \" .\"))):\n",
    "            res = \"\"\n",
    "            for character in x:\n",
    "                if character not in accepted_characters:\n",
    "                    if character in replace_dict:\n",
    "                        res += replace_dict[character]\n",
    "                else:\n",
    "                    res += character\n",
    "            f.write(res+\"\\n\")\n",
    "#     i += 1\n",
    "#     if i == 20:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbe755",
   "metadata": {},
   "source": [
    "# build ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c123b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "word2vec_model = Word2Vec.load(\"Word2VecModel\")\n",
    "# FastText_model = FastText.load(\"FastTextModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d6a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_tuple(words, model):\n",
    "    return tuple([model.wv.key_to_index[words[i]] for i in range(len(words))])\n",
    "\n",
    "# seems some of the words in the text file is not present in model key_to_index\n",
    "def build_ngrams(n):\n",
    "    ngram_counts = {}\n",
    "    with open(\"test.txt\", \"r\", encoding='utf-8') as t:\n",
    "        while True:\n",
    "            line = t.readline()\n",
    "            if line == \"\":\n",
    "                break\n",
    "            words = tokenizer.tokenize_words(line)\n",
    "            for i in range(max(len(words)-n+1, 0)):\n",
    "                try:\n",
    "#                     pre_words_indices = tuple([word2vec_model.wv.key_to_index[words[i+j]] for j in range(n)])\n",
    "                    pre_words_indices = words_to_tuple(words[i:i+n], word2vec_model)\n",
    "                    if pre_words_indices in ngram_counts:\n",
    "                        ngram_counts[pre_words_indices] += 1\n",
    "                    else:\n",
    "                        ngram_counts[pre_words_indices] = 0\n",
    "                except:\n",
    "                    pass\n",
    "    return ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27283c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words counts:  42671360\n",
      "biword counts:  34459834\n",
      "threeword counts:  19255364\n",
      "Wall time: 11min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_counts = build_ngrams(1)\n",
    "word_counts[\"all\"] = sum(word_counts.values())\n",
    "print(\"words counts: \", word_counts[\"all\"])\n",
    "# 100MB of RAM\n",
    "biword_counts = build_ngrams(2)\n",
    "biword_counts[\"all\"] = sum(biword_counts.values())\n",
    "print(\"biword counts: \", biword_counts[\"all\"])\n",
    "\n",
    "# 700MB of RAM\n",
    "threeword_counts = build_ngrams(3)\n",
    "threeword_counts[\"all\"] = sum(threeword_counts.values())\n",
    "print(\"threeword counts: \", threeword_counts[\"all\"])\n",
    "# 1.6GB of RAM\n",
    "# fourword_counts = build_ngrams(4)\n",
    "# fourword_counts[\"all\"] = sum(fourword_counts.values())\n",
    "# 3.5GB of RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c09e4",
   "metadata": {},
   "source": [
    "# make benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11e8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"faspell_main.csv\")\n",
    "# df = df[df[\"error-category\"] != 2].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d9ffcde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_neighbors = {\n",
    "    \"ض\" : [\"ص\", \"ظ\", \"ز\", \"ذ\"],\n",
    "    \"ص\" : [\"ض\", \"ث\"],\n",
    "    \"ث\" : [\"ص\", \"ق\"],\n",
    "    \"ق\" : [\"ف\", \"ث\", \"غ\"],\n",
    "    \"ف\" : [\"ق\", \"غ\"],\n",
    "    \"غ\" : [\"ف\", \"ه\", \"ق\"],\n",
    "    \"ع\" : [\"غ\", \"ه\"],\n",
    "    \"ه\" : [\"ع\", \"خ\", \"ح\"],\n",
    "    \"خ\" : [\"ه\", \"ح\"],\n",
    "    \"ح\" : [\"خ\", \"ج\", \"ه\"],\n",
    "    \"ج\" : [\"ح\", \"چ\"],\n",
    "    \"چ\" : [\"ج\", \"پ\"],\n",
    "    \"پ\" : [\"چ\", \"ب\"],\n",
    "    \"ش\" : [\"س\"],\n",
    "    \"س\" : [\"ش\", \"ی\"],\n",
    "    \"ی\" : [\"س\", \"ب\"],\n",
    "    \"ب\" : [\"ی\", \"ل\"],\n",
    "    \"ل\" : [\"ب\", \"ا\", \"آ\"],\n",
    "    \"ا\" : [\"ل\", \"ت\"],\n",
    "    \"آ\" : [\"ل\", \"ت\"],\n",
    "    \"ت\" : [\"ا\", \"ن\", \"ط\", \"آ\"],\n",
    "    \"ن\" : [\"ت\", \"م\"],\n",
    "    \"م\" : [\"ن\", \"ک\"],\n",
    "    \"ک\" : [\"م\", \"گ\"],\n",
    "    \"گ\" : [\"ک\"],\n",
    "    \"ظ\" : [\"ط\", \"ض\", \"ز\", \"ذ\"],\n",
    "    \"ط\" : [\"ظ\", \"ز\", \"ت\"],\n",
    "    \"ز\" : [\"ط\", \"ض\", \"ر\", \"ذ\"],\n",
    "    \"ژ\" : [\"ط\", \"ر\"],\n",
    "    \"ر\" : [\"ز\", \"ذ\"],\n",
    "    \"ذ\" : [\"ر\", \"ض\", \"د\", \"ذ\"],\n",
    "    \"د\" : [\"ذ\", \"ی\"],\n",
    "    \"و\" : [\"د\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5e757d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaceble_characters = [\"و\", \"ن\", \"م\", \"ل\", \"گ\", \"ک\", \"ق\", \"ف\", \"غ\", \"ع\", \"ظ\", \"ط\", \"ض\", \"ص\", \"ی\", \"ه\",\n",
    "#                         \"ش\", \"س\", \"ژ\", \"ز\", \"ر\", \"ذ\", \"د\", \"خ\", \"ح\", \"چ\", \"ج\", \"ث\", \"ت\", \"پ\", \"ب\", \"ا\", \"آ\"]\n",
    "\n",
    "replaceble_characters = list(character_neighbors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "806733fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [word2vec_model.wv.index_to_key[x[0][0]] for x in sorted(word_counts.items(), key = lambda x: x[1], reverse=True)[1:20]]\n",
    "stop_words_indices = [word2vec_model.wv.key_to_index[x] for x in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed7f82d",
   "metadata": {},
   "source": [
    "# random errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "879bf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_character_to_replace(character, close_change):\n",
    "    if close_change:\n",
    "        return random.choice(character_neighbors[character])\n",
    "    else:\n",
    "        return random.choice(list(set(replaceble_characters)-(set(character_neighbors[character]+[character]))))\n",
    "    \n",
    "def replace_character(phrase, location, close_change):\n",
    "    char = select_character_to_replace(phrase[location], close_change)\n",
    "    return phrase[:location] + char + phrase[location+1:]\n",
    "\n",
    "def del_character(phrase, location, close_change):\n",
    "    return phrase[:location] + phrase[location+1:]\n",
    "\n",
    "def add_character(phrase, location, close_change):\n",
    "    char = select_character_to_replace(phrase[location], close_change)\n",
    "    return phrase[:location] + char + phrase[location:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4e9e2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_error_function = {\n",
    "    0:replace_character,\n",
    "    1:del_character,\n",
    "    2:add_character\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fe1ff183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for replacement, 1 for deletion, 2 for addition\n",
    "def word_random_change(word, distance, close_change):\n",
    "    \n",
    "    while True:\n",
    "        locations = random.sample(range(len(word)), k=min(distance, len(word)))\n",
    "        if sum([word[location] in replaceble_characters for location in locations])==len(locations):\n",
    "            break\n",
    "\n",
    "    locations = np.array(sorted(locations))\n",
    "    changes = []\n",
    "    for i in range(len(locations)):\n",
    "        random_edit = random.randint(0, 2)\n",
    "        word = index_to_error_function[random_edit](word, locations[i], close_change)\n",
    "        if random_edit == 1:\n",
    "            if i != len(locations):\n",
    "                locations[i+1:] -= 1\n",
    "        elif random_edit == 2:\n",
    "            if i != len(locations):\n",
    "                locations[i+1:] += 1\n",
    "        changes.append(random_edit)\n",
    "    return word, changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "96bcafa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3] + random.choices([1, 2, 3, 4, 5], k=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3a96b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_random_error(phrase, number_of_errors, number_of_words_with_error, close_change):\n",
    "    words = tokenizer.tokenize_words(normalizer.normalize(phrase))\n",
    "    final_words = []\n",
    "    final_changes = []\n",
    "    error_words_indices = random.sample(range(len(words)), k=number_of_words_with_error)\n",
    "    error_words_indices = error_words_indices + random.choices(error_words_indices, k=number_of_errors-number_of_words_with_error)\n",
    "    error_words_indices, word_error_counts = np.unique(error_words_indices, return_counts=True)\n",
    "    for i in range(len(error_words_indices)):\n",
    "        if sum([character in replaceble_characters for character in words[error_words_indices[i]]])<word_error_counts[i]:\n",
    "            return None, None\n",
    "    for i in range(len(words)):\n",
    "        if i in error_words_indices:\n",
    "            error, changes = word_random_change(words[i], word_error_counts[np.where(error_words_indices==i)[0][0]], close_change)\n",
    "            final_words.append(error)\n",
    "            final_changes.append(changes)\n",
    "        else:\n",
    "            final_words.append(words[i])\n",
    "            final_changes.append([])\n",
    "            \n",
    "    return \" \".join(final_words), final_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "9212ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_error(counts, model, from_top=10000, k=500, max_error_word=2, max_error_num=3):\n",
    "    randomly_chosen_ngrams = random.sample([x[0] for x in sorted(biword_counts.items(), key=lambda x: x[1], reverse=True)[1:from_top] if \n",
    "                                         sum([y in stop_words_indices for y in x[0]])==0], k=300)\n",
    "    randomly_chosen_ngrams = [\" \".join([word2vec_model.wv.index_to_key[y] for y in x]) for x in randomly_chosen_ngrams]\n",
    "    df = pd.DataFrame({'correct word':[], \"misspelled word\":[], \"total edit distance\":[], \n",
    "                   \"number of words with error\":[], \"close change\": [], \"changes\":[]})\n",
    "    for ngram in randomly_chosen_ngrams:\n",
    "        for error_num in range(1, max_error_num+1):\n",
    "            for errored_word_num in range(1, min(error_num+1, max_error_num)):\n",
    "                for close_change in [False, True]:\n",
    "    #                 print(biword, error_num, errored_word_num, close_change)\n",
    "                    for i in range(3):\n",
    "                        misspelled_phrase, changes = phrase_random_error(ngram,error_num, errored_word_num, close_change)\n",
    "                        if misspelled_phrase==None:\n",
    "                            continue\n",
    "                        if levenshtein(misspelled_phrase, ngram) == error_num:\n",
    "                            df = df.append(pd.DataFrame({'correct word':ngram, 'misspelled word': misspelled_phrase,\n",
    "                                                         'total edit distance':error_num, 'number of words with error':errored_word_num,\n",
    "                                                         'close change':close_change, 'changes':[changes]})).reset_index(drop=True)\n",
    "                            break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "189d8abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_error_df = make_random_error(biword_counts, word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3438d084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2976\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct word</th>\n",
       "      <th>misspelled word</th>\n",
       "      <th>total edit distance</th>\n",
       "      <th>number of words with error</th>\n",
       "      <th>close change</th>\n",
       "      <th>changes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>رییس‌جمهور آمریکا</td>\n",
       "      <td>ییس‌جنهور آمریا</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[1, 0], [1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>همراه داشته</td>\n",
       "      <td>همره داشته</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[1], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>گفت پس</td>\n",
       "      <td>گفت ژ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[], [1, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>علنی دیروز</td>\n",
       "      <td>علنی دبروز</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[], [0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>موضوع مهم</td>\n",
       "      <td>موصضوع مهم</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[2], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>تحریم‌های جدید</td>\n",
       "      <td>تقریم‌هکی جدید</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0, 0], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>سه هفته</td>\n",
       "      <td>س هفته</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[1], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>محل حادثه</td>\n",
       "      <td>ح حادثه</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[1, 1], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>انجام دادند</td>\n",
       "      <td>انجام دادد</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[], [1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>آیت‌الله العظمی</td>\n",
       "      <td>آیت‌آلله العسظمی</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0], [2]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>واردات خودرو</td>\n",
       "      <td>واردات خدودذد</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[], [2, 0, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>نظر گرفتن</td>\n",
       "      <td>نضر کگزرفتن</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[0], [2, 2]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>وارد میدان</td>\n",
       "      <td>وارد میذدتن</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[], [2, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>محدود کردن</td>\n",
       "      <td>محدود کخن</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[], [1, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>رییس کل</td>\n",
       "      <td>ییس چکث</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[1], [2, 0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           correct word   misspelled word  total edit distance  \\\n",
       "767   رییس‌جمهور آمریکا   ییس‌جنهور آمریا                  3.0   \n",
       "1178        همراه داشته        همره داشته                  1.0   \n",
       "1339             گفت پس             گفت ژ                  2.0   \n",
       "1924         علنی دیروز        علنی دبروز                  1.0   \n",
       "779           موضوع مهم        موصضوع مهم                  1.0   \n",
       "372      تحریم‌های جدید    تقریم‌هکی جدید                  2.0   \n",
       "1018            سه هفته            س هفته                  1.0   \n",
       "950           محل حادثه           ح حادثه                  2.0   \n",
       "2041        انجام دادند        انجام دادد                  1.0   \n",
       "334     آیت‌الله العظمی  آیت‌آلله العسظمی                  2.0   \n",
       "1623       واردات خودرو     واردات خدودذد                  3.0   \n",
       "159           نظر گرفتن       نضر کگزرفتن                  3.0   \n",
       "2391         وارد میدان       وارد میذدتن                  2.0   \n",
       "2360         محدود کردن         محدود کخن                  2.0   \n",
       "2286            رییس کل           ییس چکث                  3.0   \n",
       "\n",
       "      number of words with error  close change          changes  \n",
       "767                          2.0           1.0    [[1, 0], [1]]  \n",
       "1178                         1.0           0.0        [[1], []]  \n",
       "1339                         1.0           0.0     [[], [1, 0]]  \n",
       "1924                         1.0           1.0        [[], [0]]  \n",
       "779                          1.0           1.0        [[2], []]  \n",
       "372                          1.0           0.0     [[0, 0], []]  \n",
       "1018                         1.0           0.0        [[1], []]  \n",
       "950                          1.0           0.0     [[1, 1], []]  \n",
       "2041                         1.0           0.0        [[], [1]]  \n",
       "334                          2.0           0.0       [[0], [2]]  \n",
       "1623                         1.0           1.0  [[], [2, 0, 0]]  \n",
       "159                          2.0           1.0    [[0], [2, 2]]  \n",
       "2391                         1.0           1.0     [[], [2, 0]]  \n",
       "2360                         1.0           0.0     [[], [1, 0]]  \n",
       "2286                         2.0           0.0    [[1], [2, 0]]  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(random_error_df))\n",
    "random_error_df.sample(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "68c1f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_error_df.to_csv(\"random bigram.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17682aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word, misspelled word, edit distance, number of words with error, close_change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f913128",
   "metadata": {},
   "source": [
    "# homophonic error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bc803653",
   "metadata": {},
   "outputs": [],
   "source": [
    "homophonic_letters = {\n",
    "    \"ض\":\n",
    "        [\"ظ\", \"ز\", \"ذ\"],\n",
    "    \"ص\":\n",
    "        [\"س\", \"ث\"],\n",
    "    \"ث\":\n",
    "        [\"س\", \"ص\"],\n",
    "    \"ق\":\n",
    "        [\"غ\"],\n",
    "    \"غ\":\n",
    "        [\"ق\"],\n",
    "    \"ه\":\n",
    "        [\"ح\"],\n",
    "    \"ح\":\n",
    "        [\"ه\"],\n",
    "    \"س\":\n",
    "        [\"ث\", \"ص\"],\n",
    "    \"ت\":\n",
    "        [\"ط\"],\n",
    "    \"ظ\":\n",
    "        [\"ض\", \"ز\", \"ذ\"],\n",
    "    \"ط\":\n",
    "        [\"ت\"],\n",
    "    \"ز\":\n",
    "        [\"ض\", \"ظ\", \"ذ\"],\n",
    "    \"ذ\":\n",
    "        [\"ض\", \"ز\", \"ظ\"],\n",
    "}\n",
    "homophonic_letters_in_one_string = \"ضصثقغهحستظطزذ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "847a68a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_homophonic_error(phrase, error_num):\n",
    "    phrase_homophonic_letters_indices = [i for i in range(len(phrase)) if phrase[i] in homophonic_letters_in_one_string]\n",
    "    if error_num > len(phrase_homophonic_letters_indices):\n",
    "        return None\n",
    "    phrase_homophonic_letters_indices = random.sample(phrase_homophonic_letters_indices, k=error_num)\n",
    "    phrase_homophonic_letters_indices = list(sorted(phrase_homophonic_letters_indices))\n",
    "    phrase_ranges = []\n",
    "    if phrase_homophonic_letters_indices[0] != 0:\n",
    "        phrase_ranges.append([phrase[0:phrase_homophonic_letters_indices[0]]])\n",
    "    phrase_ranges.append([phrase[phrase_homophonic_letters_indices[0]]] + \n",
    "                     homophonic_letters[phrase[phrase_homophonic_letters_indices[0]]])\n",
    "    for i in range(1, len(phrase_homophonic_letters_indices)):\n",
    "        phrase_ranges.append([phrase[phrase_homophonic_letters_indices[i-1]+1:phrase_homophonic_letters_indices[i]]])\n",
    "        phrase_ranges.append([phrase[phrase_homophonic_letters_indices[i]]]+homophonic_letters[phrase[phrase_homophonic_letters_indices[i]]])\n",
    "    if phrase_homophonic_letters_indices[-1] != len(phrase)-1:\n",
    "        phrase_ranges.append([phrase[phrase_homophonic_letters_indices[-1]+1:]])\n",
    "    res = list(itertools.product(*phrase_ranges))\n",
    "#     return res\n",
    "\n",
    "    return [\"\".join(x) for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "77975504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_homophonic_errors(counts, model, from_top=10000, k=500):\n",
    "    df = pd.DataFrame()\n",
    "    randomly_chosen_ngrams = random.sample([x[0] for x in sorted(counts.items(), key=lambda x: x[1], reverse=True)[1:from_top] if \n",
    "                                            sum([len(set(model.wv.index_to_key[y])&set(homophonic_letters_in_one_string))>0 for y in x[0]])>0], k=k)\n",
    "    randomly_chosen_ngrams = [\" \".join([word2vec_model.wv.index_to_key[y] for y in x]) for x in randomly_chosen_ngrams] \n",
    "    \n",
    "    df = pd.DataFrame({'correct word':[], \"misspelled word\":[], \"total edit distance\":[], \n",
    "                   \"number of words with error\":[], \"close change\": [], \"changes\":[]})\n",
    "    for phrase in randomly_chosen_ngrams:\n",
    "        phrases_homophobable_chars_num = sum([x in homophonic_letters_in_one_string for x in phrase])\n",
    "        for i in range(1, phrases_homophobable_chars_num+1):\n",
    "            errored_phrases = list(set(phrase_homophonic_error(phrase, i))-set([phrase]))\n",
    "            for errored_phrase in errored_phrases:\n",
    "                df = df.append(pd.DataFrame({'correct word':[phrase], \"misspelled word\":[errored_phrase], \n",
    "                                             \"total edit distance\":[levenshtein(phrase, errored_phrase)], \n",
    "                               \"number of words with error\":[None], \"close change\": [True], \"changes\":[0]})).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "453c2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "homophonic_df = make_homophonic_errors(biword_counts, word2vec_model, k=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "b27324d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2801\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct word</th>\n",
       "      <th>misspelled word</th>\n",
       "      <th>total edit distance</th>\n",
       "      <th>number of words with error</th>\n",
       "      <th>close change</th>\n",
       "      <th>changes</th>\n",
       "      <th>none word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>درخصوص این</td>\n",
       "      <td>درخثوس این</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>درخصوص این</td>\n",
       "      <td>درخثوص این</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>خود هستند</td>\n",
       "      <td>خود حثتند</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>بسته است</td>\n",
       "      <td>بثطه اصط</td>\n",
       "      <td>4.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>در نزدیکی</td>\n",
       "      <td>در نذدیکی</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>کاهش ارزش</td>\n",
       "      <td>کاحش ارظش</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>صیانت از</td>\n",
       "      <td>سیانت اذ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>هنری و</td>\n",
       "      <td>حنری و</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>موظف است</td>\n",
       "      <td>موذف اصت</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>گزارش داده</td>\n",
       "      <td>گزارش دادح</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>دولت گذشته</td>\n",
       "      <td>دولط گزشطه</td>\n",
       "      <td>3.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>که نباید</td>\n",
       "      <td>کح نباید</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>اتفاقاتی که</td>\n",
       "      <td>اتفاقاتی کح</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>باشگاه استقلال</td>\n",
       "      <td>باشگاه اثطقلال</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>به‌طور طبیعی</td>\n",
       "      <td>بح‌طور طبیعی</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        correct word misspelled word  total edit distance  \\\n",
       "2024      درخصوص این      درخثوس این                  2.0   \n",
       "2026      درخصوص این      درخثوص این                  1.0   \n",
       "1699       خود هستند       خود حثتند                  2.0   \n",
       "1434        بسته است        بثطه اصط                  4.0   \n",
       "367        در نزدیکی       در نذدیکی                  1.0   \n",
       "1712       کاهش ارزش       کاحش ارظش                  2.0   \n",
       "1717        صیانت از        سیانت اذ                  2.0   \n",
       "1073          هنری و          حنری و                  1.0   \n",
       "1512        موظف است        موذف اصت                  2.0   \n",
       "357       گزارش داده      گزارش دادح                  1.0   \n",
       "628       دولت گذشته      دولط گزشطه                  3.0   \n",
       "2114        که نباید        کح نباید                  1.0   \n",
       "907      اتفاقاتی که     اتفاقاتی کح                  1.0   \n",
       "1185  باشگاه استقلال  باشگاه اثطقلال                  2.0   \n",
       "699     به‌طور طبیعی    بح‌طور طبیعی                  1.0   \n",
       "\n",
       "     number of words with error close change  changes  none word  \n",
       "2024                       None         True      0.0       True  \n",
       "2026                       None         True      0.0       True  \n",
       "1699                       None         True      0.0       True  \n",
       "1434                       None         True      0.0       True  \n",
       "367                        None         True      0.0       True  \n",
       "1712                       None         True      0.0       True  \n",
       "1717                       None         True      0.0       True  \n",
       "1073                       None         True      0.0       True  \n",
       "1512                       None         True      0.0       True  \n",
       "357                        None         True      0.0       True  \n",
       "628                        None         True      0.0       True  \n",
       "2114                       None         True      0.0       True  \n",
       "907                        None         True      0.0       True  \n",
       "1185                       None         True      0.0       True  \n",
       "699                        None         True      0.0       True  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(homophonic_df))\n",
    "homophonic_df.sample(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "61748a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noneword, word error column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1379ff8",
   "metadata": {},
   "source": [
    "# add noneword change column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f952c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noneword_column(model, df):\n",
    "    noneword_column = []\n",
    "    for i in range(len(df)):\n",
    "        tokenized_words=tokenizer.tokenize_words(normalizer.normalize(df.loc[i, \"misspelled word\"]))\n",
    "        if sum([x in model.wv.key_to_index for x in tokenized_words]) == len(tokenized_words):\n",
    "            noneword_column.append(False)\n",
    "        else:\n",
    "            noneword_column.append(True)\n",
    "    return noneword_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "03e208c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "homophonic_df[\"none word\"] = get_noneword_column(word2vec_model, homophonic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "54423193",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_error_df[\"none word\"] = get_noneword_column(word2vec_model, random_error_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "747eaf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "homophonic_df.to_csv(\"homophonics biword.csv\")\n",
    "random_error_df.to_csv(\"random bigram.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
