{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82306554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import parsivar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6edf97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = parsivar.Normalizer()\n",
    "tokenizer = parsivar.Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b8471",
   "metadata": {},
   "source": [
    "# Keyhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aeb9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"Keyhan.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df667eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uniques = []\n",
    "content = None\n",
    "for key in data:\n",
    "    content = data[key][\"content\"]\n",
    "    uniques.extend([x for x in np.unique([y for y in normalizer.normalize(content)]) if x not in uniques])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c0f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "['\\n', ' ', '!', '(', ')', '.', '0', '1', '3', '5', '9', ':', '«', '\\xad', '»', '،', '؟', 'آ', 'ئ', 'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ل', 'م', 'ن', 'ه', 'و', 'پ', 'چ', 'ژ', 'ک', 'گ', 'ی', '\\u200c', '2', '4', '6', '8', '[', ']', '7', '؛', '|', 'ّ', 'ْ', '-', '٪', '#', '@', '_', 'a', 'd', 'e', 'f', 'i', 'j', 'm', 'n', 'r', 'y', 'A', 'I', 'N', 'U', '=', '٥', '*', '/', 'E', 'H', 'K', 'M', 'R', '%', 'B', 'C', 'G', 'O', 'F', '+', 'P', 'S', 'D', 'L', 'W', 'c', 'g', 'k', 'l', 'o', 'p', 't', 'u', 'v', 'w', 'T', 'b', 'h', 's', 'Ε', 'Ι', 'V', 'x', 'Z', '?', 'Y', 'J', 'X', 'Q', '&', 'z', 'ٔ', 'q', '×', ';', 'ʳ', '\"', '\\u200a', '`', '{', '}', 'ö', '÷', '\\u2005', 'é', 'ۂ', 'ٰ', '$', 'ü', 'Î', 'İ', 'è', 'ş', 'å', \"'\", '±', '\\u2001', 'ó', '\\u2009', 'â', 'ï', '°', '>', '\\\\']\n"
     ]
    }
   ],
   "source": [
    "print(len(uniques))\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6605c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حالت­خوبه\n",
      "حالت‌خوبه\n",
      "حالت خوبه\n",
      "حالت خوبه\n",
      "حالت خوبه\n",
      "حالت خوبه\n"
     ]
    }
   ],
   "source": [
    "print(\"حالت\"+\"\\xad\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200c\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200a\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2001\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2005\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2009\"+\"خوبه\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e858cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "    \"\\xad\":\"\",\n",
    "    \"\\u200a\":\" \",\n",
    "    \"\\u2001\":\" \",\n",
    "    \"\\u2005\":\" \",\n",
    "    \"\\u2009\":\" \",\n",
    "    \"ۂ\":\n",
    "        \"ه\",\n",
    "    \"ئ\": \n",
    "        \"ی\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47cc1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_characters = [0,1,2,3,4,5,6,7,8,9,\n",
    "                       \"و\", \"ن\", \"م\", \"ل\", \"گ\", \"ک\", \"ق\", \"ف\", \"غ\", \"ع\", \"ظ\", \"ط\", \"ض\", \"ص\", \"ی\", \"ه\",\n",
    "                       \"ش\", \"س\", \"ژ\", \"ز\", \"ر\", \"ذ\", \"د\", \"خ\", \"ح\", \"چ\", \"ج\", \"ث\", \"ت\", \"پ\", \"ب\", \"ا\", \"آ\",\n",
    "                       \" \", \"\\u200c\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "858abbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "f = open(\"test.txt\", \"w\", encoding='utf-8')\n",
    "for key in data:\n",
    "    for x in tokenizer.tokenize_sentences(normalizer.normalize(data[key][\"content\"].replace(\"\\n\", \".\"))):\n",
    "        res = \"\"\n",
    "        for character in x:\n",
    "            if character not in accepted_characters:\n",
    "                if character in replace_dict:\n",
    "                    res += replace_dict[character]\n",
    "            else:\n",
    "                res += character\n",
    "        f.write(res+\"\\n\")\n",
    "#         f.write(x+\"\\n\")\n",
    "        \n",
    "#     i += 1\n",
    "#     if i == 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d760981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55500 posts\n",
      "152491109 characters\n",
      "1182522 lines\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(data)} posts')\n",
    "f = open(\"test.txt\", \"r\", encoding='utf-8')\n",
    "number_of_chars = len(f.read())\n",
    "print(f'{number_of_chars} characters')\n",
    "f.close()\n",
    "f = open(\"test.txt\", \"r\", encoding='utf-8')\n",
    "number_of_lines = len(f.readlines())\n",
    "print(f'{number_of_lines} lines')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8882ea9f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_uniques = []\n",
    "content = None\n",
    "with open(\"test.txt\", \"r\", encoding='utf-8') as t:\n",
    "    while True:\n",
    "        content = t.readline()\n",
    "        if content == \"\":\n",
    "            break\n",
    "        text_uniques.extend([x for x in np.unique([y for y in content]) if x not in text_uniques])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bf87351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "['\\n', ' ', 'آ', 'ا', 'ت', 'ح', 'خ', 'د', 'ر', 'ز', 'س', 'ش', 'ع', 'ل', 'م', 'ن', 'ه', 'و', 'پ', 'چ', 'ک', 'ی', 'ب', 'ج', 'ص', 'غ', 'ف', 'ق', 'ذ', '\\u200c', 'ث', 'ض', 'ط', 'ژ', 'گ', 'ظ', '-']\n"
     ]
    }
   ],
   "source": [
    "print(len(text_uniques))\n",
    "print(text_uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe961752",
   "metadata": {},
   "source": [
    "# Hamshahri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04f96ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(\"Hamshahri.json\", \"r\"))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67096b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uniques = []\n",
    "content = None\n",
    "for key in data:\n",
    "    for key2 in data[key][\"posts\"]:\n",
    "        content = data[key][\"posts\"][key2]\n",
    "        uniques.extend([x for x in np.unique([y for y in normalizer.normalize(content)]) if x not in uniques])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46caf6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "['\\n', ' ', '-', '.', '0', '1', '2', '3', '4', '5', '7', '8', '9', ':', '«', '»', '،', '؛', 'آ', 'ئ', 'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'ف', 'ق', 'ل', 'م', 'ن', 'ه', 'و', 'پ', 'چ', 'ژ', 'ک', 'گ', 'ی', '\\u200c', '(', ')', '6', 'ذ', 'غ', 'E', 'T', 'U', 'S', '؟', '!', 'a', 'c', 'm', 's', 'A', 'B', 'J', 'M', 'N', 'O', 'R', 'V', 'e', 'i', 'n', 'p', '*', 'C', '/', 'F', 'D', 'P', 'ّ', 'd', 'f', 'h', 'k', 'l', 'o', 'r', 't', 'g', 'x', 'z', 'L', '#', 'I', 'Q', '+', 'W', '[', ']', '&', 'u', 'H', 'K', 'X', 'G', '\\u2009', 'v', 'y', 'b', '_', 'ٓ', '٥', 'ْ', '\\xad', 'Z', 'Y', 'w', '%', 'j', '@', '|', 'q', '\\u200a', 'í', '×', '{', '}', '>', '=', 'è', '?', 'é', \"'\", 'ٰ', 'ٔ', 'à', 'ç', 'ê', 'İ', 'ş', '\"', ';', 'ۆ', '\\\\', '٪', '\\x81', '\\u2005']\n"
     ]
    }
   ],
   "source": [
    "print(len(uniques))\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c61b01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "    \"\\xad\":\"\",\n",
    "    \"\\u200a\":\" \",\n",
    "    \"\\u2001\":\" \",\n",
    "    \"\\u2005\":\" \",\n",
    "    \"\\u2009\":\" \",\n",
    "    \"ۂ\":\n",
    "        \"ه\",\n",
    "    \"ئ\": \n",
    "        \"ی\",\n",
    "    \"٥\":\n",
    "        \"5\",\n",
    "    \"ۆ\": \n",
    "        \"و\",\n",
    "    \n",
    "}\n",
    "accepted_characters = [0,1,2,3,4,5,6,7,8,9,\n",
    "                       \"و\", \"ن\", \"م\", \"ل\", \"گ\", \"ک\", \"ق\", \"ف\", \"غ\", \"ع\", \"ظ\", \"ط\", \"ض\", \"ص\", \"ی\", \"ه\",\n",
    "                       \"ش\", \"س\", \"ژ\", \"ز\", \"ر\", \"ذ\", \"د\", \"خ\", \"ح\", \"چ\", \"ج\", \"ث\", \"ت\", \"پ\", \"ب\", \"ا\", \"آ\",\n",
    "                       \" \", \"\\u200c\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "470522ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حالت­خوبه\n",
      "حالتخوبه\n",
      "حالت‌خوبه\n",
      "حالت‍خوبه\n",
      "حالت‫خوبه\n",
      "حالت‬خوبه\n",
      "حالت خوبه\n",
      "حالت‪خوبه\n",
      "حالت خوبه\n",
      "حالت خوبه\n"
     ]
    }
   ],
   "source": [
    "print(\"حالت\"+\"\\xad\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\x81\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200c\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200d\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u202b\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u202c\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u200a\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u202a\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2005\"+\"خوبه\")\n",
    "print(\"حالت\"+\"\\u2009\"+\"خوبه\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6114cc2d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f = open(\"test.txt\", \"a\", encoding='utf-8')\n",
    "# i = 0\n",
    "for key in data:\n",
    "    for k in data[key][\"posts\"]:\n",
    "        for x in tokenizer.tokenize_sentences(normalizer.normalize(data[key][\"posts\"][k].replace(\"\\n\", \" .\"))):\n",
    "            res = \"\"\n",
    "            for character in x:\n",
    "                if character not in accepted_characters:\n",
    "                    if character in replace_dict:\n",
    "                        res += replace_dict[character]\n",
    "                else:\n",
    "                    res += character\n",
    "            f.write(res+\"\\n\")\n",
    "#     i += 1\n",
    "#     if i == 20:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c09e4",
   "metadata": {},
   "source": [
    "# make benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c123b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "word2vec_model = Word2Vec.load(\"Word2VecModel\")\n",
    "# FastText_model = FastText.load(\"FastTextModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e11e8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"faspell_main.csv\")\n",
    "# df = df[df[\"error-category\"] != 2].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6815d3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#misspelt</th>\n",
       "      <th>corrected</th>\n",
       "      <th>error-category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>آبری</td>\n",
       "      <td>عابری</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>آزاد يخلق</td>\n",
       "      <td>آزادي خلق</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>آزم</td>\n",
       "      <td>عازم</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>آش پز خونه</td>\n",
       "      <td>آشپزخونه</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>آش پزخانه</td>\n",
       "      <td>آشپزخانه</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4766</th>\n",
       "      <td>گروه‌هي</td>\n",
       "      <td>گروهي</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>گهان</td>\n",
       "      <td>گناهان</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4806</th>\n",
       "      <td>گورو</td>\n",
       "      <td>گروه</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4817</th>\n",
       "      <td>گيرندگانودند</td>\n",
       "      <td>گيرندگان بودند</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4846</th>\n",
       "      <td>یمتانان</td>\n",
       "      <td>سیمبانان</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         #misspelt       corrected  error-category\n",
       "4             آبری           عابری               0\n",
       "23       آزاد يخلق       آزادي خلق               0\n",
       "27             آزم            عازم               0\n",
       "36      آش پز خونه        آشپزخونه               0\n",
       "37       آش پزخانه        آشپزخانه               0\n",
       "...            ...             ...             ...\n",
       "4766       گروه‌هي           گروهي               0\n",
       "4799          گهان          گناهان               0\n",
       "4806          گورو            گروه               0\n",
       "4817  گيرندگانودند  گيرندگان بودند               0\n",
       "4846       یمتانان        سیمبانان               0\n",
       "\n",
       "[283 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"error-category\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d6a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_tuple(words, model):\n",
    "    return tuple([model.wv.key_to_index[words[i]] for i in range(len(words))])\n",
    "\n",
    "# seems some of the words in the text file is not present in model key_to_index\n",
    "def build_ngrams(n):\n",
    "    ngram_counts = {}\n",
    "    with open(\"test.txt\", \"r\", encoding='utf-8') as t:\n",
    "        while True:\n",
    "            line = t.readline()\n",
    "            if line == \"\":\n",
    "                break\n",
    "            words = tokenizer.tokenize_words(line)\n",
    "            for i in range(max(len(words)-n+1, 0)):\n",
    "                try:\n",
    "#                     pre_words_indices = tuple([word2vec_model.wv.key_to_index[words[i+j]] for j in range(n)])\n",
    "                    pre_words_indices = words_to_tuple(words[i:i+n], word2vec_model)\n",
    "                    if pre_words_indices in ngram_counts:\n",
    "                        ngram_counts[pre_words_indices] += 1\n",
    "                    else:\n",
    "                        ngram_counts[pre_words_indices] = 0\n",
    "                except:\n",
    "                    pass\n",
    "    return ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27283c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_counts = build_ngrams(1)\n",
    "word_counts[\"all\"] = sum(word_counts.values())\n",
    "# 100MB of RAM\n",
    "biword_counts = build_ngrams(2)\n",
    "biword_counts[\"all\"] = sum(biword_counts.values())\n",
    "# 700MB of RAM\n",
    "threeword_counts = build_ngrams(3)\n",
    "threeword_counts[\"all\"] = sum(threeword_counts.values())\n",
    "# 1.6GB of RAM\n",
    "# fourword_counts = build_ngrams(4)\n",
    "# fourword_counts[\"all\"] = sum(fourword_counts.values())\n",
    "# 3.5GB of RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ffcde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_neighbors = {\n",
    "    \"ض\" : [\"ص\", \"ظ\", \"ز\", \"ذ\"],\n",
    "    \"ص\" : [\"ض\", \"ث\"],\n",
    "    \"ث\" : [\"ص\", \"ق\"],\n",
    "    \"ق\" : [\"ف\", \"ث\", \"غ\"],\n",
    "    \"ف\" : [\"ق\", \"غ\"],\n",
    "    \"غ\" : [\"ف\", \"ه\", \"ق\"],\n",
    "    \"ع\" : [\"غ\", \"ه\"],\n",
    "    \"ه\" : [\"ع\", \"خ\", \"ح\"],\n",
    "    \"خ\" : [\"ه\", \"ح\"],\n",
    "    \"ح\" : [\"خ\", \"ج\", \"ه\"],\n",
    "    \"ج\" : [\"ح\", \"چ\"],\n",
    "    \"چ\" : [\"ج\", \"پ\"],\n",
    "    \"پ\" : [\"چ\", \"ب\"],\n",
    "    \"ش\" : [\"س\"],\n",
    "    \"س\" : [\"ش\", \"ی\"],\n",
    "    \"ی\" : [\"س\", \"ب\"],\n",
    "    \"ب\" : [\"ی\", \"ل\"],\n",
    "    \"ل\" : [\"ب\", \"ا\", \"آ\"],\n",
    "    \"ا\" : [\"ل\", \"ت\"],\n",
    "    \"آ\" : [\"ل\", \"ت\"],\n",
    "    \"ت\" : [\"ا\", \"ن\", \"ط\", \"آ\"],\n",
    "    \"ن\" : [\"ت\", \"م\"],\n",
    "    \"م\" : [\"ن\", \"ک\"],\n",
    "    \"ک\" : [\"م\", \"گ\"],\n",
    "    \"گ\" : [\"ک\"],\n",
    "    \"ظ\" : [\"ط\", \"ض\", \"ز\", \"ذ\"],\n",
    "    \"ط\" : [\"ظ\", \"ز\", \"ت\"],\n",
    "    \"ز\" : [\"ط\", \"ض\", \"ر\", \"ذ\"],\n",
    "    \"ژ\" : [\"ط\", \"ر\"],\n",
    "    \"ر\" : [\"ز\", \"ذ\"],\n",
    "    \"ذ\" : [\"ر\", \"ض\", \"د\", \"ذ\"],\n",
    "    \"د\" : [\"ذ\", \"ی\"],\n",
    "    \"و\" : [\"د\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e757d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaceble_characters = [\"و\", \"ن\", \"م\", \"ل\", \"گ\", \"ک\", \"ق\", \"ف\", \"غ\", \"ع\", \"ظ\", \"ط\", \"ض\", \"ص\", \"ی\", \"ه\",\n",
    "#                         \"ش\", \"س\", \"ژ\", \"ز\", \"ر\", \"ذ\", \"د\", \"خ\", \"ح\", \"چ\", \"ج\", \"ث\", \"ت\", \"پ\", \"ب\", \"ا\", \"آ\"]\n",
    "\n",
    "replaceble_characters = list(character_neighbors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "879bf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_character_to_replace(phrase, location):\n",
    "    if random.random() >= 0.1:\n",
    "        return random.choice(character_neighbors[phrase[location]])\n",
    "    else:\n",
    "        return random.choice(list(set(replaceble_characters)-set(character_neighbors[phrase[location]])))\n",
    "    \n",
    "def replace_character(phrase, location):\n",
    "    char = select_character_to_replace(phrase, location)\n",
    "    return phrase[:location] + char + phrase[location+1:]\n",
    "\n",
    "def del_character(phrase, location):\n",
    "    return phrase[:location] + phrase[location+1:]\n",
    "\n",
    "def add_character(phrase, location):\n",
    "    if random.random() >= 0.1:\n",
    "        return phrase[:location] + random.choice(character_neighbors[phrase[location]]) + phrase[location:]\n",
    "    else:\n",
    "        return phrase[:location] + random.choice(list(set(replaceble_characters)-set(character_neighbors[phrase[location]]))) + phrase[location:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "bbf28337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spell_errors(phrase):\n",
    "    misspelled_words = []\n",
    "    while True:\n",
    "        error_locations = random.sample(range(len(phrase)), k=2)\n",
    "        i=0\n",
    "        while i < len(error_locations):\n",
    "            if phrase[error_locations[i]] == \" \" or phrase[error_locations[i]] == \"\\u200c\" or phrase[error_locations[i]] == \"-\":\n",
    "                break\n",
    "            i+=1\n",
    "#             print(i)\n",
    "        if i == len(error_locations):\n",
    "            break\n",
    "    error_locations = list(sorted(error_locations))\n",
    "    for location in error_locations:\n",
    "        misspelled_words.extend([replace_character(phrase, location), add_character(phrase, location), del_character(phrase, location)])\n",
    "    misspelled_words.append(replace_character(replace_character(phrase, error_locations[0]), error_locations[1]))\n",
    "    misspelled_words.append(add_character(replace_character(phrase, error_locations[0]), error_locations[1]))\n",
    "    misspelled_words.append(add_character(replace_character(phrase, error_locations[0]), error_locations[1]))\n",
    "    \n",
    "    misspelled_words.append(replace_character(add_character(phrase, error_locations[0]), error_locations[1]+1))\n",
    "    misspelled_words.append(add_character(add_character(phrase, error_locations[0]), error_locations[1]+1))\n",
    "    misspelled_words.append(del_character(add_character(phrase, error_locations[0]), error_locations[1]+1))\n",
    "    \n",
    "    misspelled_words.append(replace_character(del_character(phrase, error_locations[0]), error_locations[1]-1))\n",
    "    misspelled_words.append(add_character(del_character(phrase, error_locations[0]), error_locations[1]-1))\n",
    "    misspelled_words.append(del_character(del_character(phrase, error_locations[0]), error_locations[1]-1))\n",
    "    return misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "806733fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [word2vec_model.wv.index_to_key[x[0][0]] for x in sorted(word_counts.items(), key = lambda x: x[1], reverse=True)[1:20]]\n",
    "stop_words_indices = [word2vec_model.wv.key_to_index[x] for x in stop_words]\n",
    "sorted_bigrams = sorted(biword_counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "e8a20b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams with frequency more than 100 and not containing stopwords\n",
    "top_bigrams_without_stopwords = [x for x in [x for x in sorted_bigrams if x[1] > 100] if x[0][0] not in stop_words_indices and x[0][1] not in stop_words_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "ba186faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_manipulate = [word2vec_model.wv.index_to_key[x[0][0]] + \" \" + word2vec_model.wv.index_to_key[x[0][1]] for x in random.sample(top_bigrams_without_stopwords, k=1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "4de62883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_to_manilupate[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b50fb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'correct word':[], \"misspelled word\":[]})\n",
    "for word in words_to_manilupate:\n",
    "    misspelled_words = create_spell_errors(word)\n",
    "    df = df.append(pd.DataFrame({'correct word':[word]*len(misspelled_words), 'misspelled word': misspelled_words})).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "14317304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "faef2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"misspelled bigram words.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
